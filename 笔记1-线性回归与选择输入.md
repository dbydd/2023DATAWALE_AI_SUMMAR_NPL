# 什么是线性回归

线性回归就是根据输入拟合一个多项式函数，此处的输入即为样本中特征的数量，这些所谓的“特征”，其实就是由操作者所选择的量化指标，而所要拟合的多项式通常会是线性多项式的形式。

例如对于衡量一栋房子的价格(y)，我们可以从它的长$x_1$宽$x_2$,以及地段等级$x_3$入手，那么我们采集到的样本可能会是如下情况：

| $x_1$ | $x_2$ | $x_3$ | y          |
| ----- | ----- | ----- | ---------- |
| 10    | 10    | 2     | $1.186e6$  |
| 12    | 14    | 1     | $1.2123e7$ |
| 11    | 11    | 1     | $1.8162e7$ |
| 17    | 12    | 2     | $2.2131e6$ |
| 20    | 61    | 2     | $4.012e6$  |
| 18    | 9     | 3     | $0.882e6$  |
| 12    | 20    | 1     | $1.2321e7$ |

正如上文所说，我们要拟合的是一个**线性多项式，也就是如下形式**

$$
y(x_1...xn) = \Sigma^n_{i=1} c_nx_n + b
$$

若是以线性代数的视角观看，也可以是如下形式：

$$
Cx +b = y:\begin{bmatrix}c_1 \dots c_n\end{bmatrix}\begin{bmatrix}x_1 \\ \vdots \\ x_n\end{bmatrix} + b = y
$$

这是理想状态，但实际工程中最不常见的就是理想状态。

注：为什么没有高次项？因为这算在输入里，我们默认平方的操作在获取样本的时候就做好了

注：对于多输出的情况，左侧的 C 会变成一个矩阵，右侧的 y 会成为一列向量，b 也会对应的成为一列向量

对于 fit()函数，实际上就是通过这些样本来选择恰当的 C 与 b，这一切都是通过 Loss 函数来实现的，所谓 Loss 函数，就是使用类似于牛顿迭代法的方式来迭代这两者，只不过在更高维度的情况下，牛顿迭代法会变成**梯度下降法**，但其实其中思想其实是一样的。

对于 Loss 函数来说，x 反而是常量，变量是 C 与 b，也就是:

$$
Loss(C,b) = Cx+b - y
$$

回顾上文，当 C 与 b 取理想值时 Loss 的值恒为 0，当然实际上这样多半是过拟合了

# 选择输入

正如我所说，实际工程中最不常见的就是理想状态，查看上面的样本，我们很容易就能发现一个问题：y 相比于 x 太大了，在两者比值如此悬殊的情况下，毫无疑问的会使拟合结果中的 b 变得过大，c 变得过小，从而产生无法接受的误差，这就是这一小节的标题：选择输入所要做的事情。

通常，我们在实际工程中选择输入的时候会进行“样本清洗”，在这里，我们可以提前将 y 取对数后乘 10，这样就能使样本再次平衡。

| $x_1$ | $x_2$ | $x_3$ | y         |
| ----- | ----- | ----- | --------- |
| 10    | 10    | 2     | $1.186$   |
| 12    | 14    | 1     | $12.1237$ |
| 11    | 11    | 1     | $18.162$  |
| 17    | 12    | 2     | $2.131$   |
| 20    | 61    | 2     | $4.012$   |
| 18    | 9     | 3     | $0.882$   |
| 12    | 20    | 1     | $12.321$  |

# 输入数据的污染，偏差

毫无疑问的，实际工程中采集的样本会带有污染条目与偏差成分，我们理想中的数据拟合，要做的是拟合出一条能够通过各个样本点的 n 维曲线，n 个维度对应了 n 个特征。但是在有污染与偏差的情况下可万万不能这么做，在我看来这也就是过拟合的来源。

怎么避免过拟合？这是 ai 界长久以来的研究话题，对于线性回归，在经过学习后我也有一些不成熟的想法，在这里写在笔记上：

我们从最小二乘开始，也就是

$$
C^\mathbf{T} C x = C^\mathbf{T}\hat{y} + C^\mathbf{T}(e) \\
e = y - \hat{y} = e_1 + e_2 + \dots + e_n
$$

这里的 e，实际上是误差向量，我们所做的事情就是将等式两边左乘 C 的转置，其中奥妙会体现在右边，样本所组成的向量空间中，有用的数据通常处于同一处方向，没用的数据则来源于另一些基向量，也就是说两者将会互相垂直，如图所示：

![1692349098657](image/笔记1-线性回归与选择输入/1692349098657.png "(随手画的，有点丑)")

高中时我们就知道了这一点：任意两个互相垂直的向量点乘时结果为 0，这就是最小二乘的奥妙，通过最小二乘，我们不仅能将输入的误差消除，还能将 y 向量的误差成分也消除！

但想必你也有疑惑：为什么这两个玩意垂直？

这就得从 PCA（主成分分析）说起了，简而言之：这两玩意就是垂直，这是由于有用的成分与误差成分的比例导致的，误差向量相对于主向量更小，于是会出现这样的场景

是的，重点就在于误差向量是可以被提取出来的，也许读到上面你会觉得不屑一顾，因为实际工程中不仅仅消除不了误差，我们为了保证模型的通用性还会常常特意引入误差，这就是 PCA 的意义：我们确实应当保留一些误差，来让用户知道这是机器学习，而不是纯粹的理论。既然如此，为什么我们不主动控制误差向量的占比呢？

这就是我的笔记与我的思考，如上。
